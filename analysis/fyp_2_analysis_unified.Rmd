---
title: "fyp_2_analysis_unified"
output: html_document
---
```{r}
library(rjson)
library(ggplot2)
library(tidyr)
library(dplyr)
library(lme4)
library(lmerTest)
library(boot)
library(stringr)
```

#Read files:
```{r}
data_location = "../data/fyp_2_data/" 
files = list.files(path = data_location,pattern="data_subject_.*.json")
modular_data = vector()
polygon_data = vector()
hybrid_data = vector()
#data frame building
subject = vector()
prompt = vector()
correct_response = vector()
rt = vector()
response = vector()
condition = vector()
trial_index = vector()
score = vector()

j = 1
for (i in 1:length(files)) {
  path = paste(data_location,files[i],sep="")
  print(i)
  print(path)
  c = file(path, "r")
  l = readLines(c, -1L)
  close(c)
  these_data = lapply(X=l, fromJSON)
  if (these_data[1][[1]][[1]]$trial_type == "instructions") {
      this_condition = "modular"
      modular_data[length(modular_data) + 1] = these_data[1]
  } else {
    if (grepl("subtract",gsub(" ","",these_data[1][[1]][[1]]$instructions))) { 
        this_condition = "hybrid"
        hybrid_data[length(hybrid_data) + 1] = these_data[1]
    }
    else {
       this_condition = "polygon"
       polygon_data[length(polygon_data) + 1] = these_data[1]
    }
  }
  
  for (trial_i in 1:length(these_data[1][[1]])) {
    if(grepl("question",gsub(" ","",these_data[1][[1]][[trial_i]]$trial_type))) {
      subject[j] = i
      condition[j] = this_condition
      prompt[j] = these_data[1][[1]][[trial_i]]$question
      correct_response[j] = these_data[1][[1]][[trial_i]]$correct_response
      
      if (is.null(these_data[1][[1]][[trial_i]]$response)) {
        response[j] = NA
      }
      else {
        response[j] = these_data[1][[1]][[trial_i]]$response
      }
      
      if (!is.na(response[j]) && response[j] == correct_response[j]) {
        score[j] = 1
      } else {
        score[j] = 0        
      }
      rt[j] = these_data[1][[1]][[trial_i]]$rt
      trial_index[j] = trial_i
      
      j = j+1
    }
  }
}
```

Write data for scoring
----------------------------

```{r}
#DONE --exact answers, explanations not yet scored
#n=9 operation questions
#write.table(complete_data[(grepl("Whatis8",gsub(" ","",complete_data$question)) | grepl("Whatis4",gsub(" ","",complete_data$question)))& grepl("explaininwords",gsub(" ","",complete_data$question)) ,c(1,3,4,5,7)],"N9_operation_to_score.csv",sep=",")

#DONE --exact answers, explanations not yet scored
#n=9 generator questions
# write.table(complete_data[grepl("generatorunder",gsub(" ","",complete_data$question)) & (grepl("nonagon",gsub(" ","",complete_data$question)) | grepl("+<sub>9",gsub(" ","",complete_data$question)) ),c(1,3,4,5,7)],"N9_generator_to_score.csv",sep=",")

#DONE --exact answers, explanations not yet scored
#n=9 inverse with explanations
#write.table(complete_data[grepl("inverseof3",gsub(" ","",complete_data$question)) & (grepl("nonagon",gsub(" ","",complete_data$question)) | grepl("+<sub>9",gsub(" ","",complete_data$question)) ),c(1,4,5,7)],"N9_inverse_to_score.csv",sep=",")

#DONE --exact answers, explanations not yet scored
#generator T/F and A/S/N with explanations
#write.table(complete_data[grepl("explaininwords",gsub(" ","",complete_data$question)) & (grepl("Trueorfalse",gsub(" ","",complete_data$question)) | grepl("always,sometimes,ornever",gsub(" ","",complete_data$question))),c(1,4,5,7)],"TF_ASN_explained_to_score.csv",sep=",")

#DONE -- exact answers, explanations not yet scored
#n-1,n-x
#write.table(complete_data[grepl("formula",gsub(" ","",complete_data$question)),c(1,4,5,7)],"inverse_formula_to_score.csv",sep=",")

#Highest math class completed, 1 if algebra 2/trig/stats or higher, 0 else
#write.table(complete_data[grepl('mathclass',gsub(' ','',complete_data$question)),c(1,4,5,7)],"highest_math_class_to_score.csv",sep=",")
```

data frame building
------------------------
```{r}
complete_data = data.frame(subject=factor(subject),condition=condition,question=prompt,correct_response=correct_response,response=response,rt=rt,trial_index=trial_index,score=score) 

#Insert scored data
inverse_formula_scores = read.csv(paste(data_location,"inverse_formula_scored.csv",sep=""))
complete_data[inverse_formula_scores$row.names,]$score = inverse_formula_scores$score

TF_ASN_explained_scores = read.csv(paste(data_location,"TF_ASN_explained_scored.csv",sep=""))
complete_data[TF_ASN_explained_scores$row.names,]$score = TF_ASN_explained_scores$score

N9_operation_scores =read.csv(paste(data_location,"N9_operation_scored.csv",sep=""))
complete_data[N9_operation_scores$row.names,]$score = N9_operation_scores$score
N9_operation_scores[grepl("Whatis4",gsub(" ","",N9_operation_scores$question)),]$correct_response = 5
N9_operation_scores[grepl("Whatis8",gsub(" ","",N9_operation_scores$question)),]$correct_response = 3
complete_data[N9_operation_scores$row.names,]$correct_response = N9_operation_scores$correct_response #omitted because of answer explanation

N9_generator_scores = read.csv(paste(data_location,"N9_generator_scored.csv",sep=""))
complete_data[N9_generator_scores$row.names,]$score = N9_generator_scores$score

N9_inverse_scores =read.csv(paste(data_location,"N9_inverse_scored.csv",sep=""))
complete_data[N9_inverse_scores$row.names,]$score = N9_inverse_scores$score
complete_data[N9_inverse_scores$row.names,]$correct_response = 6 #omitted because of answer explanation

#Strict scoring -- Some questions have "partial credit" of 0.5 for almost correct answers
complete_data = complete_data %>% mutate(score_strict = floor(score)) %>% 
  mutate(polygon=grepl("polygon",condition),prompt=grepl("prompt",condition)) #Factors of condition


```

Change dummy coding to compare hybrid and polygon to modular.
```{r}
contrasts(complete_data$condition) = cbind(c(0,0,1),c(1,0,0))
colnames(contrasts(complete_data$condition)) = c('polygon','hybrid')
contrasts(complete_data$condition) 
```

Subject Demographics
-----------------

```{r}
subject_data = complete_data %>% 
  filter(grepl('education',gsub(" ","",question)) | grepl('yourgender',gsub(" ","",question)) ) %>%
  mutate(demographic_type = ifelse(grepl('education',gsub(" ","",question)),'education','gender')) %>%
  select(subject,demographic_type,response,polygon) %>%
  mutate(response = gsub(' ','',response)) %>%
  spread(demographic_type,response) %>%
  mutate(education_high = ifelse(grepl('Bachelors',education),1,ifelse(grepl('Masters',education),1,ifelse(grepl('graduate',education),1,ifelse(grepl('Doctorate',education),1,0)))))

highest_math_classes =read.csv(paste(data_location,"highest_math_class_scored.csv",sep=""))
subject_data$math_high = highest_math_classes$math_high
```

Subject exclusion
----------------------
Exclude subjects who responded 4 or higher on familiarity with modular arithmetic or mathematical groups: they know too much

```{r}
complete_unfiltered_data = complete_data
subjects_to_exclude = unique(complete_data[grepl("Howfamiliarareyou",gsub(" ","",complete_data$question)) & (grepl("4",complete_data$response) | grepl("5",complete_data$response)),]$subject)

complete_data = complete_unfiltered_data %>% filter(!(subject %in% subjects_to_exclude))


```



Unified model - all 3 (Planned analysis)
---------------

```{r}
complete_data = complete_data %>% mutate(question_type = rep(NA,nrow(complete_data))) %>% inner_join(subject_data)

#Operation questions
complete_data[grepl("Remember,toperform",gsub(" ","",complete_data$question)),]$question_type = 'a_operation_6' #a is so it will appear at the beginning of models as reference level, to make life easier than manually altering the contrasts
complete_data[!(grepl("identity",gsub(" ","",complete_data$question)) | grepl("inverse",gsub(" ","",complete_data$question)) | grepl("generator",gsub(" ","",complete_data$question))) & (grepl("nonagon",gsub(" ","",complete_data$question)) | grepl("+<sub>9",gsub(" ","",complete_data$question)) ),]$question_type = 'operation_9'

#Inverse Questions -- Order 6
complete_data[grepl("Remember,theinverse",gsub(" ","",complete_data$question)),]$question_type = 'inverse_nonzero_6'
complete_data[grepl("Remember,theinverse",gsub(" ","",complete_data$question)) & complete_data$correct_response == '0',]$question_type = 'inverse_zero_6'
  
#Inverse Questions -- Order 9
complete_data[grepl("Whatistheinverse",gsub(" ","",complete_data$question)) & !grepl("Remember,theinverse",gsub(" ","",complete_data$question)),]$question_type = 'inverse_nonzero_9'
complete_data[grepl("Whatistheinverse",gsub(" ","",complete_data$question)) & !grepl("Remember,theinverse",gsub(" ","",complete_data$question)) & gsub(" ","",complete_data$correct_response) == '0',]$question_type = 'inverse_zero_9' 

#Inverse Questions -- order n
complete_data[inverse_formula_scores$row.names,]$question_type = 'inverse_formula_n'

#Generator Questions -- 6
complete_data[grepl("generatorunder",gsub(" ","",complete_data$question)) & (grepl("hexagon",gsub(" ","",complete_data$question)) | grepl("+<sub>6",gsub(" ","",complete_data$question)) ),]$question_type = 'generator_false_6'
complete_data[grepl("generatorunder",gsub(" ","",complete_data$question)) & (grepl("hexagon",gsub(" ","",complete_data$question)) | grepl("+<sub>6",gsub(" ","",complete_data$question)) ) & gsub(" ","",correct_response) == 'Yes',]$question_type = 'generator_true_6'

#Generator Questions -- 9
complete_data[grepl("generatorunder",gsub(" ","",complete_data$question)) & (grepl("nonagon",gsub(" ","",complete_data$question)) | grepl("+<sub>9",gsub(" ","",complete_data$question)) ),]$question_type = 'generator_false_9'
complete_data[grepl("generatorunder",gsub(" ","",complete_data$question)) & (grepl("nonagon",gsub(" ","",complete_data$question)) | grepl("+<sub>9",gsub(" ","",complete_data$question)) ) & (grepl('5',gsub(" ","",complete_data$question)) | grepl('2',gsub(" ","",complete_data$question))),]$question_type = 'generator_true_9'

#Generator T/F Questions -- n
complete_data[grepl("Trueorfalse",gsub(" ","",complete_data$question)),]$question_type = 'generator_TF_n'

#Generator A/S/N Questions -- n
complete_data[grepl("always,sometimes,ornever",gsub(" ","",complete_data$question)),]$question_type = 'generator_ASN_n'
```

```{r}
model_data = complete_data %>% filter(!is.na(question_type))
model_data$question_type = factor(model_data$question_type)
```

```{r}
mod = lmer(data=model_data, score_strict ~ question_type*condition+math_high+(1|subject))
```

```{r}
summary(mod)
```


Difference in means confidence intervals (bootstrapped)
```{r}
questions = unique(model_data$question_type)
bootstrap_delta_mean_95CI = function(data_1,data_2,num_iterations=1000,na.rm=TRUE) {
  results = rep(0,num_iterations)
  for (i in 1:num_iterations) {
    n1 = length(data_1)
    n2 = length(data_2)
    results[i] = mean(sample(data_1,n1,replace=TRUE),na.rm=na.rm)-mean(sample(data_2,n2,replace=TRUE),na.rm=na.rm)
  }
  return(quantile(results,probs=c(0.025,0.975)))
}

for (q_i in 1:length(questions)) {
  print(questions[q_i])
  boot_delta_means = bootstrap_delta_mean_95CI(model_data[model_data$question_type == questions[q_i] & model_data$condition=='polygon',]$score_strict,model_data[model_data$question_type == questions[q_i] & model_data$condition=='modular',]$score_strict)
  print('Polygon - Modular:')
  print(boot_delta_means)
  boot_delta_means = bootstrap_delta_mean_95CI(model_data[model_data$question_type == questions[q_i] & model_data$condition=='hybrid',]$score_strict,model_data[model_data$question_type == questions[q_i] & model_data$condition=='modular',]$score_strict)
  print('Hybrid - Modular:')
  print(boot_delta_means)
}
```

Post-hoc 
------------------------------------


###Hierarchical modeling of hybrid condition data
```{r}
math_high_coefficient = summary(mod)$coefficients[16,1]
modular_coefficients = summary(mod)$coefficients[c(1:13),1]
polygon_coefficients = summary(mod)$coefficients[c(14,17:28),1]+modular_coefficients
perfect_combination_coefficients = pmax(polygon_coefficients,modular_coefficients)
```

```{r}
#print(paste(lapply(levels(model_data$question_type),function(x) return(paste(x,'=(question_type==\'',x,'\')',sep=''))),collapse=', '))
# print(paste(lapply(1:length(levels(model_data$question_type)),function(x) return(paste('coeffs[',x,']*',levels(model_data$question_type)[x],sep=''))),collapse='+'))
hierarchical_data = model_data %>% filter(condition == 'hybrid') %>%
  mutate(a_operation_6=(question_type=='a_operation_6'), generator_ASN_n=(question_type=='generator_ASN_n'), generator_false_6=(question_type=='generator_false_6'), generator_false_9=(question_type=='generator_false_9'), generator_TF_n=(question_type=='generator_TF_n'), generator_true_6=(question_type=='generator_true_6'), generator_true_9=(question_type=='generator_true_9'), inverse_formula_n=(question_type=='inverse_formula_n'), inverse_nonzero_6=(question_type=='inverse_nonzero_6'), inverse_nonzero_9=(question_type=='inverse_nonzero_9'), inverse_zero_6=(question_type=='inverse_zero_6'), inverse_zero_9=(question_type=='inverse_zero_9'), operation_9=(question_type=='operation_9'))

#Augment the data with weights and indicators for E-M algorithm
augmenting_frame = data.frame(theta=c(T,F,F),phi=c(NA,T,F),weight=c(1,1,1))
augmented_data = merge(hierarchical_data,augmenting_frame,by=NULL) 

#Subjects
hybrid_subjects = unique(hierarchical_data$subject)
```


```{r}
log_likelihood_of_data = function(data,coeffs) { #Given a fixed set of coefficients, calculates the log-likelihood of the data
  model = lm(data=data,score ~ 0+offset(math_high_coefficient*math_high+coeffs[1]+coeffs[2]*generator_ASN_n+coeffs[3]*generator_false_6+coeffs[4]*generator_false_9+coeffs[5]*generator_TF_n+coeffs[6]*generator_true_6+coeffs[7]*generator_true_9+coeffs[8]*inverse_formula_n+coeffs[9]*inverse_nonzero_6+coeffs[10]*inverse_nonzero_9+coeffs[11]*inverse_zero_6+coeffs[12]*inverse_zero_9+coeffs[13]*operation_9))
  return(logLik(model))
}

log_likelihood_of_full_data = function(data,theta,phi) {
  LL = 0.0
  for (subj in hybrid_subjects) {
    perfect_weight = theta*exp(log_likelihood_of_data(hierarchical_data[hierarchical_data$subject == subj,],perfect_combination_coefficients)[1])
    polygon_weight = (1-theta)*phi*exp(log_likelihood_of_data(hierarchical_data[hierarchical_data$subject == subj,],polygon_coefficients)[1])
    modular_weight = (1-theta)*(1-phi)*exp(log_likelihood_of_data(hierarchical_data[hierarchical_data$subject == subj,],modular_coefficients)[1])
    LL = LL+log(perfect_weight+polygon_weight+modular_weight)
  }
  return(LL)
}
```


Naive -- How well is data explained by assuming everybody just did one of three possibilities?
```{r}
print('polygon: ')
print(log_likelihood_of_full_data(hierarchical_data,0,1))
print('modular: ')
print(log_likelihood_of_full_data(hierarchical_data,0,0))
print('50-50 modular polygon: ')
print(log_likelihood_of_full_data(hierarchical_data,0,0.5))
print('max of the two: ')
print(log_likelihood_of_full_data(hierarchical_data,1,0))

```

Expectation-maximization
```{r}
#max number of iterations
num_iterations = 100
delta_LL_threshold =  0.0001
#initial values - uniform
theta = 0.5
phi = 0.5 

LL_hist = vector()

LL_hist[1] = log_likelihood_of_full_data(hierarchical_data,theta,phi)

for (iter in 1:num_iterations) {
  #E step
  for (subj in hybrid_subjects) {
    perfect_weight = theta*exp(log_likelihood_of_data(hierarchical_data[hierarchical_data$subject == subj,],perfect_combination_coefficients)[1])
    polygon_weight = (1-theta)*phi*exp(log_likelihood_of_data(hierarchical_data[hierarchical_data$subject == subj,],polygon_coefficients)[1])
    modular_weight = (1-theta)*(1-phi)*exp(log_likelihood_of_data(hierarchical_data[hierarchical_data$subject == subj,],modular_coefficients)[1])
    sum_weights = perfect_weight+polygon_weight+modular_weight
    augmented_data[augmented_data$subject == subj & augmented_data$theta,]$weight = perfect_weight/sum_weights
    augmented_data[augmented_data$subject == subj & !augmented_data$theta & augmented_data$phi,]$weight = polygon_weight/sum_weights
    augmented_data[augmented_data$subject == subj & !augmented_data$theta & !augmented_data$phi,]$weight = modular_weight/sum_weights
  }
  
  #M step
  theta = sum(augmented_data[augmented_data$theta,]$weight)/sum(augmented_data$weight)
  phi = sum(augmented_data[!augmented_data$theta & (augmented_data$phi==TRUE),]$weight)/sum(augmented_data[!augmented_data$theta,]$weight)
  
  #LL and termination
  LL_hist[iter+1] = log_likelihood_of_full_data(hierarchical_data,theta,phi)
  print(paste("theta: ",theta,", phi: ",phi,', LL: ',LL_hist[iter+1]))
  if (LL_hist[iter+1]-LL_hist[iter] < delta_LL_threshold) {
    print("Difference of log likelihood from previous step is below threshold, terminating.")
    break
  }
}

```



###Can we identify condition based on words used in explanations?
This would be useful to get an idea of what strategy hybrid subjects are using from their explanations
```{r}
model_data = model_data %>% mutate(polygon_word_count = str_count(response,'move') + str_count(response,'spaces') + str_count(response,'clockwise') + str_count(response,'steps') + str_count(response,'count') + str_count(response,'arrow') + str_count(response,'hexagon') + str_count(response,'nonagon') + str_count(response,'ngon') + str_count(response,'spots') + str_count(response,'positions'),modular_word_count = str_count(response,'add') + str_count(response,'subtract') + str_count(response,'plus') + str_count(response,'minus') + str_count(response,'[+]')) %>% mutate(polygon_wc_higher = polygon_word_count > modular_word_count,modular_wc_higher = modular_word_count > polygon_word_count+1,both_wc_high=(polygon_word_count >= 1 & modular_word_count >= 1))
```

```{r}
xtabs(~condition+polygon_wc_higher,data=model_data[grepl('explain',gsub(' ','',model_data$question)),])
xtabs(~condition+modular_wc_higher,data=model_data[grepl('explain',gsub(' ','',model_data$question)),])
xtabs(~condition+both_wc_high,data=model_data[grepl('explain',gsub(' ','',model_data$question)),])
xtabs(~condition+score_strict+polygon_wc_higher,data=model_data[grepl('explain',gsub(' ','',model_data$question)) & model_data$question_type=='inverse_nonzero_9',])
xtabs(~condition+score_strict+modular_wc_higher,data=model_data[grepl('explain',gsub(' ','',model_data$question)) & model_data$question_type=='inverse_nonzero_9',])
xtabs(~condition+score_strict+both_wc_high,data=model_data[grepl('explain',gsub(' ','',model_data$question)) & model_data$question_type=='inverse_nonzero_9',])
xtabs(~condition+score_strict+polygon_wc_higher,data=model_data[grepl('explain',gsub(' ','',model_data$question)) & model_data$question_type=='generator_true_9',])
xtabs(~condition+score_strict+modular_wc_higher,data=model_data[grepl('explain',gsub(' ','',model_data$question)) & model_data$question_type=='generator_true_9',])
xtabs(~condition+score_strict+both_wc_high,data=model_data[grepl('explain',gsub(' ','',model_data$question)) & model_data$question_type=='generator_true_9',])
```
